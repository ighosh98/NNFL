{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBusW5fj-TdU",
        "colab_type": "text"
      },
      "source": [
        "**NNFL Course Project** <br>\n",
        "**Paper** :  Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts (Paper ID: 211)<br>\n",
        "**Members**<br>\n",
        "Indraneel Ghosh 2016B1A70938P<br>\n",
        "Mohit Kriplani 2016B1A70870P<br>\n",
        "Himanshu Agarwal 2016B3A30570P <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax7QhLAz-0VJ",
        "colab_type": "text"
      },
      "source": [
        "**Guidelines:**\n",
        "1.   Use Tensorflow, PyTorch or Keras to implement the given CharSCNN architecture given in the paper.\n",
        "\n",
        "2.  For training: either use STS or SSTb. No need to compare with other baseline models, as specified in the paper.\n",
        "3.  Report accuracy of your model using pre-trained word embeddings.\n",
        "\n",
        "**Dataset Link**: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZWscAMyoFxW",
        "colab_type": "text"
      },
      "source": [
        "References for Keras:\n",
        "https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n",
        "\n",
        "https://www.kaggle.com/coder98/blood-cell-subtype-identification-cudnn-cnn-rnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f2jwF6T-36Q",
        "colab_type": "text"
      },
      "source": [
        "Note: Modify all functions using Keras as a framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9Oo2Dhec99",
        "colab_type": "text"
      },
      "source": [
        "Loading Files to Cloud(Do not run again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Thx4yVJfAJf",
        "colab_type": "text"
      },
      "source": [
        "File upload Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJqZuLI57B6J",
        "colab_type": "text"
      },
      "source": [
        "https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ygidp7NfEDv",
        "colab_type": "text"
      },
      "source": [
        "Code starts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pppXraQa-w_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import subprocess\n",
        "from glob import glob\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import nltk\n",
        "from skimage import measure\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import scipy.ndimage as ndi\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input, concatenate, Reshape, LSTM, CuDNNLSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw6DmQLqOeJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read(filename):\n",
        "    col_names=['A1','A2','A3','A4','A5','A6']\n",
        "    dat=pd.read_csv(filename,names=col_names,header=None)\n",
        "    lines=dat['A1'].to_numpy()+dat['A6'].to_numpy()\n",
        "    #Mohit Kriplani\n",
        "    words_map = {}\n",
        "    word_cnt = 0\n",
        "    \n",
        "    k_wrd = 5 #check\n",
        "\n",
        "    y = [] \n",
        "    x_wrd = []\n",
        "\n",
        "    max_sen_len, num_sent = 0, 20000 # Max sentence length and number of sentences\n",
        "    # f = open(input, 'r')\n",
        "    # lines = f.readlines()\n",
        "    \n",
        "    for line in lines[:num_sent]:\n",
        "        words = line[:-1].split()\n",
        "        tokens = words[1:]\n",
        "        y.append(int(float(words[0])))\n",
        "        max_sen_len = max(max_sen_len,len(tokens))\n",
        "        for token in tokens:\n",
        "            if token not in words_map:\n",
        "                words_map[token] = word_cnt\n",
        "                word_cnt += 1\n",
        "    \n",
        "    for line in lines[:num_sent]:\n",
        "        words = line[:-1].split()\n",
        "        tokens = words[1:]\n",
        "        word_mat = [0] * (max_sen_len+k_wrd-1)\n",
        "\n",
        "        for i in xrange(len(tokens)):\n",
        "            word_mat[(k_wrd/2)+i] = words_map[tokens[i]]\n",
        "        x_wrd.append(word_mat)\n",
        "    max_sen_len += k_wrd-1\n",
        "    # y: 1 or 0 (i.e., positive or negative)\n",
        "    data = (num_sent, word_cnt, max_sen_len, k_wrd, x_wrd, y)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_E5NL1EQMhN",
        "colab_type": "text"
      },
      "source": [
        "**Search for Equivalent library functions before you start writing code**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FJ90BrMPPI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbedIDLayer(object):\n",
        "    \"\"\"\n",
        "    Efficient linear function for one-hot input.\n",
        "    \"\"\"\n",
        "    #Kriplani\n",
        "    def __init__(\n",
        "        self,\n",
        "        rng,\n",
        "        input=None,\n",
        "        n_input=None,\n",
        "        n_output=None,\n",
        "        W=None,\n",
        "    ):\n",
        "        if input is None:\n",
        "            input = T.imatrix('x')\n",
        "\n",
        "        if W is None:\n",
        "            W_values = np.asarray(\n",
        "                rng.uniform(low=-np.sqrt(6.0/(n_input+n_output)),\n",
        "                            high=np.sqrt(6.0/(n_input+n_output)),\n",
        "                            size=(n_input, n_output)),\n",
        "                dtype=theano.config.floatX)\n",
        "\n",
        "            # W_values[0,0] = 0\n",
        "            W_tmp = theano.shared(value=W_values, name='W', borrow=True)\n",
        "        else:\n",
        "            W_values = W.astype(theano.config.floatX)\n",
        "            W_tmp = theano.shared(value=W_values, name='W', borrow=True)\n",
        "\n",
        "        self.W = W_tmp\n",
        "        self.output = self.W[input]\n",
        "        self.params = [self.W]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmXIKOFpyYnG",
        "colab_type": "text"
      },
      "source": [
        "Fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM8QkHNJPXTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedLayer(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rng,\n",
        "        input=None, \n",
        "        n_input=784,\n",
        "        n_output=10,\n",
        "        activation=None,\n",
        "        W=None,\n",
        "        b=None\n",
        "    ):\n",
        "\n",
        "        self.input = input\n",
        "\n",
        "        if W is None:\n",
        "            W_values = np.asarray(\n",
        "                rng.uniform(low=-np.sqrt(6.0/(n_input+n_output)),\n",
        "                            high=np.sqrt(6.0/(n_input+n_output)),\n",
        "                            size=(n_input, n_output)),\n",
        "                dtype=theano.config.floatX)\n",
        "            if activation == sigmoid:\n",
        "                W_values *= 4.0\n",
        "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
        "\n",
        "        if b is None:\n",
        "            b_values = np.zeros((n_output,), dtype=theano.config.floatX)\n",
        "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
        "\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        linear_output = T.dot(input, self.W) + self.b\n",
        "\n",
        "        if activation is None:\n",
        "            self.output = linear_output\n",
        "        else:\n",
        "            self.output = activation(linear_output)\n",
        "\n",
        "        self.params = [self.W, self.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTcOJ2SLPfyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvolutionalLayer(object):\n",
        "  #Indraneel\n",
        "    def __init__(\n",
        "        self,\n",
        "        rng,\n",
        "        input,\n",
        "        filter_shape=None,\n",
        "        image_shape=None,\n",
        "        activation=relu\n",
        "    ):\n",
        "        self.input = input\n",
        "        self.rng = rng\n",
        "\n",
        "        \"\"\"filter shape = n_feature_map, in channel, width, height\"\"\"\n",
        "        \"\"\"channel * width * height\"\"\"\n",
        "        fan_in = np.prod(filter_shape[1:]) # 1*2*3\n",
        "        \"\"\"feature map \"\"\"\n",
        "        fan_out = filter_shape[0] * np.prod(filter_shape[2:]) # 0*2*3\n",
        "        \n",
        "        W_bound = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "        self.W = theano.shared(\n",
        "            np.asarray(\n",
        "                self.rng.uniform(\n",
        "                    low = -W_bound,\n",
        "                    high = W_bound,\n",
        "                    size = filter_shape\n",
        "                ),\n",
        "                dtype = theano.config.floatX\n",
        "            ),\n",
        "            borrow = True\n",
        "        )\n",
        "\n",
        "        b_values = np.zeros((filter_shape[0],),\n",
        "                            dtype=theano.config.floatX)\n",
        "        self.b = theano.shared(b_values, borrow=True)\n",
        "\n",
        "        conv_out = conv.conv2d(\n",
        "            input=self.input,\n",
        "            filters=self.W,\n",
        "            filter_shape=filter_shape,\n",
        "            image_shape=image_shape\n",
        "        )\n",
        "\n",
        "        self.output = activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "\n",
        "        self.params = [self.W, self.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF_YBvFbyLdm",
        "colab_type": "text"
      },
      "source": [
        "Maxpooling exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8E7bs77Pjy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaxPoolingLayer(object):\n",
        "  #Indraneel\n",
        "    def __init__(self, input, poolsize=(2,2)):\n",
        "        pooled_out = downsample.pool_2d(\n",
        "            input=input,\n",
        "            ws=poolsize,\n",
        "            ignore_border=True\n",
        "        )\n",
        "        self.output = pooled_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42s9lCr9P-Ry",
        "colab_type": "text"
      },
      "source": [
        "Keras should have an in-built optimizer function. Please verify this.(Indraneel)[All optimizzers exist]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5mkmGARA1MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer(object):\n",
        "    def __init__(self, params=None):\n",
        "        if params is None:\n",
        "            return NotImplementedError()\n",
        "        self.params = params\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        if loss is None:\n",
        "            return NotImplementedError()\n",
        "\n",
        "        self.updates = OrderedDict()\n",
        "        self.gparams = [T.grad(loss, param) for param in self.params]\n",
        "\n",
        "# SGD exists\n",
        "class SGD(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, params=None):\n",
        "        super(SGD, self).__init__(params=params)\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(SGD, self).updates(loss=loss)\n",
        "\n",
        "        for param, gparam in zip(self.params, self.gparams):\n",
        "            self.updates[param] = param - self.learning_rate * gparam\n",
        "\n",
        "        return self.updates\n",
        "#SGD Momentum exists\n",
        "class MomentumSGD(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n",
        "        super(MomentumSGD, self).__init__(params=params)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(MomentumSGD, self).updates(loss=loss)\n",
        "\n",
        "        for v, param, gparam in zip(self.vs, self.params, self.gparams):\n",
        "            _v = v * self.momentum\n",
        "            _v = _v - self.learning_rate * gparam\n",
        "            self.updates[param] = param + _v\n",
        "            self.updates[v] = _v\n",
        "\n",
        "        return self.updates\n",
        "#Adagrad exists\n",
        "class AdaGrad(Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, eps=1e-6, params=None):\n",
        "        super(AdaGrad, self).__init__(params=params)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eps = eps\n",
        "        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(AdaGrad, self).updates(loss=loss)\n",
        "\n",
        "        for accugrad, param, gparam\\\n",
        "        in zip(self.accugrads, self.params, self.gparams):\n",
        "            agrad = accugrad + gparam * gparam\n",
        "            dx = - (self.learning_rate / T.sqrt(agrad + self.eps)) * gparam\n",
        "            self.updates[param] = param + dx\n",
        "            self.updates[accugrad] = agrad\n",
        "\n",
        "        return self.updates\n",
        "#RMSprop exists\n",
        "class RMSprop(Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n",
        "        super(RMSprop, self).__init__(params=params)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.alpha = alpha\n",
        "        self.eps = eps\n",
        "\n",
        "        self.mss = [build_shared_zeros(t.shape.eval(),'ms') for t in self.params]\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(RMSprop, self).updates(loss=loss)\n",
        "\n",
        "        for ms, param, gparam in zip(self.mss, self.params, self.gparams):\n",
        "            _ms = ms*self.alpha\n",
        "            _ms += (1 - self.alpha) * gparam * gparam\n",
        "            self.updates[ms] = _ms\n",
        "            self.updates[param] = param - self.learning_rate * gparam / T.sqrt(_ms + self.eps)\n",
        "\n",
        "        return self.updates\n",
        "\n",
        "# adadelta exists\n",
        "class AdaDelta(Optimizer):\n",
        "    def __init__(self, rho=0.95, eps=1e-6, params=None):\n",
        "        super(AdaDelta, self).__init__(params=params)\n",
        "\n",
        "        self.rho = rho\n",
        "        self.eps = eps\n",
        "        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n",
        "        self.accudeltas = [build_shared_zeros(t.shape.eval(),'accudelta') for t in self.params]\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(AdaDelta, self).updates(loss=loss)\n",
        "\n",
        "        for accugrad, accudelta, param, gparam\\\n",
        "        in zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n",
        "            agrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n",
        "            dx = - T.sqrt((accudelta + self.eps)/(agrad + self.eps)) * gparam\n",
        "            self.updates[accudelta] = (self.rho*accudelta + (1 - self.rho) * dx * dx)\n",
        "            self.updates[param] = param + dx\n",
        "            self.updates[accugrad] = agrad\n",
        "\n",
        "        return self.updates\n",
        "\n",
        "#Adam exists\n",
        "class Adam(Optimizer):\n",
        "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1-1e-8, params=None):\n",
        "        super(Adam, self).__init__(params=params)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.b1 = beta1\n",
        "        self.b2 = beta2\n",
        "        self.gamma = gamma\n",
        "        self.t = theano.shared(np.float32(1))\n",
        "        self.eps = eps\n",
        "\n",
        "        self.ms = [build_shared_zeros(t.shape.eval(), 'm') for t in self.params]\n",
        "        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n",
        "\n",
        "    def updates(self, loss=None):\n",
        "        super(Adam, self).updates(loss=loss)\n",
        "        self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n",
        "\n",
        "        for m, v, param, gparam \\\n",
        "        in zip(self.ms, self.vs, self.params, self.gparams):\n",
        "            _m = self.b1_t * m + (1 - self.b1_t) * gparam\n",
        "            _v = self.b2 * v + (1 - self.b2) * gparam ** 2\n",
        "\n",
        "            m_hat = _m / (1 - self.b1 ** self.t)\n",
        "            v_hat = _v / (1 - self.b2 ** self.t)\n",
        "\n",
        "            self.updates[param] = param - self.alpha*m_hat / (T.sqrt(v_hat) + self.eps)\n",
        "            self.updates[m] = _m\n",
        "            self.updates[v] = _v\n",
        "        self.updates[self.t] = self.t + 1.0\n",
        "\n",
        "        return self.updates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeelfX2IPujA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Himanshu\n",
        "class Result(object):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def negative_log_likelihood(self):\n",
        "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
        "        return -T.mean(T.log(self.prob_of_y_given_x)[T.arange(self.y.shape[0]), self.y])\n",
        "\n",
        "    def cross_entropy(self):\n",
        "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
        "        return T.mean(T.nnet.categorical_crossentropy(self.prob_of_y_given_x, self.y))\n",
        "\n",
        "    def mean_squared_error(self):\n",
        "        return T.mean((self.x - self.y) ** 2)\n",
        "\n",
        "    def errors(self):\n",
        "        if self.y.ndim != self.y_pred.ndim:\n",
        "            raise TypeError('y should have the same shape as self.y_pred',\n",
        "                            ('y', self.y.type, 'y_pred', self.y_pred.type))\n",
        "\n",
        "        if self.y.dtype.startswith('int'):\n",
        "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
        "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
        "            return T.mean(T.neq(self.y_pred, self.y))\n",
        "        else:\n",
        "            return NotImplementedError()\n",
        "\n",
        "    def accuracy(self):\n",
        "        if self.y.dtype.startswith('int'):\n",
        "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
        "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
        "            return T.mean(T.eq(self.y_pred, self.y))\n",
        "        else:\n",
        "            return NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6IevHqQ7UK",
        "colab_type": "text"
      },
      "source": [
        "**To be done once everything above this text is finished**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvPphAnZ6s1p",
        "colab_type": "text"
      },
      "source": [
        "**CharCNN Architecture Contents**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEB2G5zBBKVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharSCNN(object):\n",
        "    def __init__(self,rng,batchsize=100,activation=relu):\n",
        "        \n",
        "        import char_load\n",
        "        (num_sent, char_cnt, word_cnt, max_word_len, max_sen_len,k_chr, k_wrd, x_chr, x_wrd, y) = char_load.read(\"tweets_clean.txt\")\n",
        "\n",
        "        dim_word = 30\n",
        "        dim_char = 5\n",
        "        cl_word = 300\n",
        "        cl_char = 50\n",
        "        k_word = k_wrd\n",
        "        k_char = k_chr\n",
        "\n",
        "        data_train_word,data_test_word,data_train_char,data_test_char,target_train,target_test = train_test_split(x_wrd, x_chr, y, random_state=1234, test_size=0.1)\n",
        "\n",
        "        x_train_word = theano.shared(np.asarray(data_train_word, dtype='int16'), borrow=True)\n",
        "        x_train_char = theano.shared(np.asarray(data_train_char, dtype='int16'), borrow=True)\n",
        "        y_train = theano.shared(np.asarray(target_train, dtype='int8'), borrow=True)\n",
        "        x_test_word = theano.shared(np.asarray(data_test_word, dtype='int16'), borrow=True)\n",
        "        x_test_char = theano.shared(np.asarray(data_test_char, dtype='int16'), borrow=True)\n",
        "        y_test = theano.shared(np.asarray(target_test, dtype='int8'), borrow=True)\n",
        "\n",
        "        self.n_train_batches = x_train_word.get_value(borrow=True).shape[0] / batchsize\n",
        "        self.n_test_batches = x_test_word.get_value(borrow=True).shape[0] / batchsize        \n",
        "        \n",
        "        \"\"\"symbol definition\"\"\"\n",
        "        index = T.iscalar()\n",
        "        x_wrd = T.wmatrix('x_wrd')\n",
        "        x_chr = T.wtensor3('x_chr')\n",
        "        y = T.bvector('y')\n",
        "        train = T.iscalar('train')\n",
        "\n",
        "        \"\"\"network definition\"\"\"\n",
        "        layer_char_embed_input = x_chr#.reshape((batchsize, max_sen_len, max_word_len))\n",
        "\n",
        "        layer_char_embed = EmbedIDLayer(rng,layer_char_embed_input,n_input=char_cnt,n_output=dim_char)\n",
        "\n",
        "        layer1_input = layer_char_embed.output.reshape((batchsize*max_sen_len, 1, max_word_len, dim_char))\n",
        "\n",
        "        layer1 = ConvolutionalLayer(rng,layer1_input,filter_shape=(cl_char, 1, k_char, dim_char),image_shape=(batchsize*max_sen_len, 1, max_word_len, dim_char))\n",
        "\n",
        "        layer2 = MaxPoolingLayer(layer1.output,poolsize=(max_word_len-k_char+1, 1))\n",
        "\n",
        "        layer_word_embed_input = x_wrd #.reshape((batchsize, max_sen_len))\n",
        "\n",
        "        layer_word_embed = EmbedIDLayer(rng,layer_word_embed_input,n_input=word_cnt,n_output=dim_word)\n",
        "\n",
        "        layer3_word_input = layer_word_embed.output.reshape((batchsize, 1, max_sen_len, dim_word))\n",
        "        layer3_char_input = layer2.output.reshape((batchsize, 1, max_sen_len, cl_char))\n",
        "\n",
        "        layer3_input = T.concatenate([layer3_word_input,layer3_char_input],axis=3)#.reshape((batchsize, 1, max_sen_len, dim_word+cl_char))\n",
        "\n",
        "        layer3 = ConvolutionalLayer(rng,layer3_input,filter_shape=(cl_word, 1, k_word, dim_word + cl_char),image_shape=(batchsize, 1, max_sen_len, dim_word + cl_char),activation=activation)\n",
        "\n",
        "        layer4 = MaxPoolingLayer(layer3.output,poolsize=(max_sen_len-k_word+1, 1))\n",
        "\n",
        "        layer5_input = layer4.output.reshape((batchsize, cl_word))\n",
        "\n",
        "        layer5 = FullyConnectedLayer(rng,dropout(rng, layer5_input, train),n_input=cl_word,n_output=50,activation=activation)\n",
        "\n",
        "        layer6_input = layer5.output\n",
        "\n",
        "        layer6 = FullyConnectedLayer(rng,dropout(rng, layer6_input, train, p=0.1),n_input=50,n_output=2,activation=None)\n",
        "\n",
        "        result = Result(layer6.output, y)\n",
        "        loss = result.negative_log_likelihood()\n",
        "        accuracy = result.accuracy()\n",
        "        params = layer6.params+layer5.params+layer3.params+layer_word_embed.params+layer1.params+layer_char_embed.params\n",
        "        updates = RMSprop(learning_rate=0.001, params=params).updates(loss)\n",
        "\n",
        "        self.train_model = theano.function(inputs=[index],outputs=[loss, accuracy],updates=updates,\n",
        "            givens={\n",
        "                x_wrd: x_train_word[index*batchsize: (index+1)*batchsize],\n",
        "                x_chr: x_train_char[index*batchsize: (index+1)*batchsize],\n",
        "                y: y_train[index*batchsize: (index+1)*batchsize],\n",
        "                train: np.cast['int32'](1)\n",
        "            }\n",
        "        )\n",
        "\n",
        "        self.test_model = theano.function(inputs=[index],outputs=[loss, accuracy],\n",
        "            givens={\n",
        "                x_wrd: x_test_word[index*batchsize: (index+1)*batchsize],\n",
        "                x_chr: x_test_char[index*batchsize: (index+1)*batchsize],\n",
        "                y: y_test[index*batchsize: (index+1)*batchsize],\n",
        "                train: np.cast['int32'](0)\n",
        "            }\n",
        "        )\n",
        "\n",
        "#Indraneel\n",
        "    def train_and_test(self, n_epoch=100):\n",
        "        epoch = 0\n",
        "        accuracies = []\n",
        "        while epoch < n_epoch:\n",
        "            epoch += 1\n",
        "            sum_loss = 0\n",
        "            sum_accuracy = 0\n",
        "            for batch_index in xrange(self.n_train_batches):\n",
        "                batch_loss, batch_accuracy = self.train_model(batch_index)\n",
        "                sum_loss = 0\n",
        "                sum_accuracy = 0\n",
        "                for batch_index in xrange(self.n_test_batches):\n",
        "                    batch_loss, batch_accuracy = self.test_model(batch_index)\n",
        "                    sum_loss += batch_loss\n",
        "                    sum_accuracy += batch_accuracy\n",
        "                loss = sum_loss / self.n_test_batches\n",
        "                accuracy = sum_accuracy / self.n_test_batches\n",
        "                accuracies.append(accuracy)\n",
        "\n",
        "                print('epoch: {}, test mean loss={}, test accuracy={}'.format(epoch, loss, accuracy))\n",
        "                print('')\n",
        "        return accuracies\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYosqw431rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Rough work for Keras model\n",
        "model = Sequential()\n",
        "model.add(Embedding(char_cnt,dim_char,input_length=char_cnt))#Input embedding char embedding\n",
        "model.add(Conv2D(filters=c1_char, kernel_size=(k_char,dim_char),input_shape=(batchsize*max_sen_len, max_word_len, dim_char))) \n",
        "#https://stackoverflow.com/questions/30633181/the-output-size-of-theano-tensor-nnet-conv-conv2d\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhXRDLSc5Pbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tPCf1m45PME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ConvolutionalLayer(rng,layer1_input,filter_shape=(cl_char, 1, k_char, dim_char),image_shape=(batchsize*max_sen_len, 1, max_word_len, dim_char))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2C-4Wr3r7w",
        "colab_type": "text"
      },
      "source": [
        "**Work Left : Main function and Visualization Functions(Using Matplotlib)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daeds2q-7q-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    random_state = 1234\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    charscnn = CharSCNN(rng, batchsize=10, activation=relu)\n",
        "    charscnn.train_and_test(n_epoch=3)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}