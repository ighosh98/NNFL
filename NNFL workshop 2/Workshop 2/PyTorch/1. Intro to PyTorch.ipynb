{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/variable1.png\" width=700px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras vs PyTorch\n",
    "\n",
    "* Keras is without a doubt the easier option if you want a plug & play framework: to quickly build, train, and evaluate a model, without spending much time on mathematical implementation details.\n",
    "\n",
    "* PyTorch offers a lower-level approach and more flexibility for the more mathematically-inclined users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head to Head\n",
    "\n",
    "Consider this head-to-head comparison of how a simple convolutional network is defined in Keras and PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 10) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        x = F.log_softmax(self.fc1(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply \"neurons.\" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up : NN using Numpy\n",
    "\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32367469.82308378\n",
      "1 30213654.033267383\n",
      "2 29355704.3500377\n",
      "3 25815573.36926423\n",
      "4 19374276.645189714\n",
      "5 12325216.509257356\n",
      "6 7090832.946296574\n",
      "7 4012322.404615649\n",
      "8 2419626.302415304\n",
      "9 1609220.3807920348\n",
      "10 1175771.736896637\n",
      "11 920585.7133917359\n",
      "12 753261.2769728169\n",
      "13 633028.3876135072\n",
      "14 540885.4088667277\n",
      "15 467177.1214497932\n",
      "16 406637.35403219773\n",
      "17 356072.68464167323\n",
      "18 313382.5690868772\n",
      "19 277049.9611778905\n",
      "20 245929.33419401833\n",
      "21 219139.60458726002\n",
      "22 195922.46250555408\n",
      "23 175748.09986019402\n",
      "24 158103.07529619941\n",
      "25 142615.04722600098\n",
      "26 128986.78637558421\n",
      "27 116959.81530870823\n",
      "28 106305.2471080716\n",
      "29 96824.81874753907\n",
      "30 88368.49658628323\n",
      "31 80805.04174253772\n",
      "32 74026.60125607796\n",
      "33 67936.77344313945\n",
      "34 62448.41397026608\n",
      "35 57492.187081824806\n",
      "36 52993.19570141002\n",
      "37 48915.74632720436\n",
      "38 45211.9036607757\n",
      "39 41841.05975688773\n",
      "40 38767.98657177525\n",
      "41 35965.85082092442\n",
      "42 33402.39850443636\n",
      "43 31064.677077012726\n",
      "44 28920.01669031983\n",
      "45 26952.826535994518\n",
      "46 25144.480623647316\n",
      "47 23478.887606482513\n",
      "48 21943.283305996687\n",
      "49 20524.8063660649\n",
      "50 19213.131677249774\n",
      "51 17998.68873432149\n",
      "52 16872.901143310803\n",
      "53 15828.686283965842\n",
      "54 14859.078131556056\n",
      "55 13958.488949576285\n",
      "56 13120.482409672026\n",
      "57 12339.682419813682\n",
      "58 11611.967483487653\n",
      "59 10933.669531046602\n",
      "60 10300.992814526933\n",
      "61 9709.927072140214\n",
      "62 9157.503105205484\n",
      "63 8640.591950068945\n",
      "64 8156.87399318029\n",
      "65 7703.933626563261\n",
      "66 7279.278450238326\n",
      "67 6880.923520109334\n",
      "68 6506.982325379447\n",
      "69 6155.871991084519\n",
      "70 5825.764694289415\n",
      "71 5515.535617383634\n",
      "72 5223.9947729698\n",
      "73 4949.5746634350535\n",
      "74 4691.111195246934\n",
      "75 4447.575473917394\n",
      "76 4218.0545962445485\n",
      "77 4001.6047302627558\n",
      "78 3797.4173642231813\n",
      "79 3604.7014297483283\n",
      "80 3422.8600031894384\n",
      "81 3251.2131754461625\n",
      "82 3089.027764736409\n",
      "83 2935.7444283282\n",
      "84 2790.842410840911\n",
      "85 2653.7162095025396\n",
      "86 2523.9549756794013\n",
      "87 2401.170680832093\n",
      "88 2284.87290934615\n",
      "89 2174.726778653123\n",
      "90 2070.3558301163785\n",
      "91 1971.422217202372\n",
      "92 1877.6228402499842\n",
      "93 1788.722832455642\n",
      "94 1704.3493317960317\n",
      "95 1624.2823130965662\n",
      "96 1548.2953537281774\n",
      "97 1476.146462349826\n",
      "98 1407.6246313275042\n",
      "99 1342.5532569142563\n",
      "100 1280.7129562155426\n",
      "101 1221.9440397412923\n",
      "102 1166.0958520309541\n",
      "103 1112.977032414758\n",
      "104 1062.5021177572285\n",
      "105 1014.4487083092472\n",
      "106 968.7329754457461\n",
      "107 925.2297658794597\n",
      "108 883.812899111304\n",
      "109 844.3901329838643\n",
      "110 806.8579254464337\n",
      "111 771.1033700678684\n",
      "112 737.045548500279\n",
      "113 704.588371705218\n",
      "114 673.6565609622621\n",
      "115 644.1729057044554\n",
      "116 616.0607665318032\n",
      "117 589.2567710362476\n",
      "118 563.6944168664991\n",
      "119 539.3134582361909\n",
      "120 516.0524705271746\n",
      "121 493.8611256560358\n",
      "122 472.68256536615405\n",
      "123 452.46273154446766\n",
      "124 433.17013743214295\n",
      "125 414.748761278444\n",
      "126 397.1568856133396\n",
      "127 380.35121818139027\n",
      "128 364.3015421680513\n",
      "129 348.9682965707956\n",
      "130 334.31368682314803\n",
      "131 320.3073254694368\n",
      "132 306.9234165241679\n",
      "133 294.13280967492915\n",
      "134 281.8999431746153\n",
      "135 270.2008052259269\n",
      "136 259.01242282295743\n",
      "137 248.31268635494158\n",
      "138 238.07943516309336\n",
      "139 228.29380241700764\n",
      "140 218.9239672996509\n",
      "141 209.95853655630268\n",
      "142 201.37812992155608\n",
      "143 193.16585691716853\n",
      "144 185.3027858492045\n",
      "145 177.77581397858364\n",
      "146 170.5695502813497\n",
      "147 163.66676899929803\n",
      "148 157.05575740949791\n",
      "149 150.7237201258575\n",
      "150 144.66038270466728\n",
      "151 138.85012748984502\n",
      "152 133.28310904200066\n",
      "153 127.94930526196279\n",
      "154 122.83762260148288\n",
      "155 117.937828560576\n",
      "156 113.24170298929289\n",
      "157 108.74179069172428\n",
      "158 104.42672597319945\n",
      "159 100.28960455532591\n",
      "160 96.32355479943963\n",
      "161 92.51922396460628\n",
      "162 88.87098380491635\n",
      "163 85.37141608642196\n",
      "164 82.0149913542336\n",
      "165 78.7948839372048\n",
      "166 75.70586067853951\n",
      "167 72.74215948594329\n",
      "168 69.89873657369687\n",
      "169 67.17041516970693\n",
      "170 64.55208361401274\n",
      "171 62.038881788141985\n",
      "172 59.62653715272508\n",
      "173 57.3111614775265\n",
      "174 55.08849226609007\n",
      "175 52.95525244311443\n",
      "176 50.90681750246109\n",
      "177 48.93964510433755\n",
      "178 47.052204307929614\n",
      "179 45.23917358068613\n",
      "180 43.497099983835525\n",
      "181 41.82431713160052\n",
      "182 40.218007391172115\n",
      "183 38.67484946880356\n",
      "184 37.19232177111153\n",
      "185 35.76847012842754\n",
      "186 34.40024507607064\n",
      "187 33.085697182176375\n",
      "188 31.823141450903172\n",
      "189 30.609939541650494\n",
      "190 29.443942202384445\n",
      "191 28.323361628994405\n",
      "192 27.24649291080038\n",
      "193 26.211548196159292\n",
      "194 25.21688909589909\n",
      "195 24.261025819800807\n",
      "196 23.342058687636154\n",
      "197 22.458537748291896\n",
      "198 21.609390137256025\n",
      "199 20.792965629563646\n",
      "200 20.008099181418697\n",
      "201 19.253472707208623\n",
      "202 18.527875690987507\n",
      "203 17.830193021753068\n",
      "204 17.15929585342936\n",
      "205 16.514084019698966\n",
      "206 15.893756339264094\n",
      "207 15.297155888343193\n",
      "208 14.723256435953001\n",
      "209 14.17129198919714\n",
      "210 13.640485850735987\n",
      "211 13.129919153721373\n",
      "212 12.638767737214897\n",
      "213 12.166318949099313\n",
      "214 11.711842894250495\n",
      "215 11.274565835707122\n",
      "216 10.853934507910807\n",
      "217 10.449268842229088\n",
      "218 10.059956969459432\n",
      "219 9.685362116512131\n",
      "220 9.32490121205728\n",
      "221 8.978107348025826\n",
      "222 8.644436606493928\n",
      "223 8.32326221286592\n",
      "224 8.014405151029633\n",
      "225 7.71705739391801\n",
      "226 7.430846510566398\n",
      "227 7.15539171127865\n",
      "228 6.89030206702987\n",
      "229 6.6351364106742174\n",
      "230 6.3896041065645015\n",
      "231 6.153261653012371\n",
      "232 5.925735829436402\n",
      "233 5.706739591564714\n",
      "234 5.495962207401826\n",
      "235 5.29305860690288\n",
      "236 5.0977123978156\n",
      "237 4.90966113474956\n",
      "238 4.728636792165483\n",
      "239 4.554373269955908\n",
      "240 4.386607235645394\n",
      "241 4.225087513131418\n",
      "242 4.0695842014410335\n",
      "243 3.91986161103866\n",
      "244 3.7756929160959976\n",
      "245 3.6369079099795876\n",
      "246 3.5032519241803435\n",
      "247 3.3745606965247688\n",
      "248 3.250643962335676\n",
      "249 3.131322222145458\n",
      "250 3.016418866267868\n",
      "251 2.905790982608833\n",
      "252 2.7992447871960993\n",
      "253 2.6966552183381314\n",
      "254 2.5978550166834093\n",
      "255 2.502699381162384\n",
      "256 2.4110484686529645\n",
      "257 2.322807793636937\n",
      "258 2.2378045272122993\n",
      "259 2.155937660188102\n",
      "260 2.0770903254754702\n",
      "261 2.0011471594034216\n",
      "262 1.9280100226701096\n",
      "263 1.8575611323741534\n",
      "264 1.7897042602535345\n",
      "265 1.7243597507662276\n",
      "266 1.6614202451059208\n",
      "267 1.600777183240266\n",
      "268 1.5423697559372023\n",
      "269 1.4861135403214105\n",
      "270 1.4319210067540857\n",
      "271 1.3797119123732355\n",
      "272 1.3294235178484008\n",
      "273 1.2809729638062135\n",
      "274 1.2343085326963603\n",
      "275 1.1893477725964963\n",
      "276 1.1460635720194134\n",
      "277 1.1043502315536826\n",
      "278 1.0641621914173185\n",
      "279 1.0254399727454124\n",
      "280 0.988131436804604\n",
      "281 0.9521913696073775\n",
      "282 0.9175692989085145\n",
      "283 0.8842074201802955\n",
      "284 0.8520629053508656\n",
      "285 0.8210957838785914\n",
      "286 0.7912603492973502\n",
      "287 0.7625136137891197\n",
      "288 0.734816197053845\n",
      "289 0.7081329641625416\n",
      "290 0.6824247152041752\n",
      "291 0.6576522324539253\n",
      "292 0.6337813016175817\n",
      "293 0.6107837332623711\n",
      "294 0.5886258366868984\n",
      "295 0.5672727206465891\n",
      "296 0.5466980350482207\n",
      "297 0.5268710678084562\n",
      "298 0.507767372769376\n",
      "299 0.4893596530805565\n",
      "300 0.471621464029316\n",
      "301 0.45452915054903476\n",
      "302 0.43806128093594365\n",
      "303 0.4221905384234005\n",
      "304 0.4068963052236495\n",
      "305 0.39215796002787495\n",
      "306 0.37795916837638965\n",
      "307 0.3642737985272072\n",
      "308 0.3510850105014157\n",
      "309 0.33837567059359686\n",
      "310 0.3261281541698243\n",
      "311 0.31432592486211675\n",
      "312 0.30295186359266546\n",
      "313 0.29199151279402885\n",
      "314 0.28142843680828716\n",
      "315 0.27124964070082314\n",
      "316 0.26143902836911903\n",
      "317 0.2519842811675715\n",
      "318 0.24287283460229667\n",
      "319 0.234092787021337\n",
      "320 0.22563014304106255\n",
      "321 0.21747467866811032\n",
      "322 0.20961388744998408\n",
      "323 0.20203852144664738\n",
      "324 0.1947381713025952\n",
      "325 0.18770161955405779\n",
      "326 0.18092125900988637\n",
      "327 0.1743857194996528\n",
      "328 0.16808702801530567\n",
      "329 0.16201603654375396\n",
      "330 0.15616502809241528\n",
      "331 0.1505261973480945\n",
      "332 0.1450917541143817\n",
      "333 0.13985357726370962\n",
      "334 0.13480811922969005\n",
      "335 0.1299422899145369\n",
      "336 0.125251906148091\n",
      "337 0.1207317382360141\n",
      "338 0.11637514518742709\n",
      "339 0.11217576639486257\n",
      "340 0.10812802953781495\n",
      "341 0.10422704208646343\n",
      "342 0.10046681481866217\n",
      "343 0.09684257760178305\n",
      "344 0.09334969546286921\n",
      "345 0.0899828037523901\n",
      "346 0.08673763929817314\n",
      "347 0.0836094995572439\n",
      "348 0.08059429437149912\n",
      "349 0.07768805275815882\n",
      "350 0.07488732611600218\n",
      "351 0.07218785522801328\n",
      "352 0.06958523883862343\n",
      "353 0.06707663474857388\n",
      "354 0.06465905113969829\n",
      "355 0.06232848780987169\n",
      "356 0.060081951048132715\n",
      "357 0.05791691953585913\n",
      "358 0.05582969572472447\n",
      "359 0.05381805374794901\n",
      "360 0.05187868537490745\n",
      "361 0.050009370709987275\n",
      "362 0.04820750873711102\n",
      "363 0.04647117857640387\n",
      "364 0.04479700274096414\n",
      "365 0.04318320520558988\n",
      "366 0.04162755292264301\n",
      "367 0.0401281740799504\n",
      "368 0.038682998784899905\n",
      "369 0.03728970170330455\n",
      "370 0.035946887274120674\n",
      "371 0.03465235714154491\n",
      "372 0.033404560386128115\n",
      "373 0.03220166591638515\n",
      "374 0.031042156422785946\n",
      "375 0.029924535925431366\n",
      "376 0.02884726174393285\n",
      "377 0.02780866876552677\n",
      "378 0.026807520609674066\n",
      "379 0.025842461203764648\n",
      "380 0.024912203931416292\n",
      "381 0.024015452280911138\n",
      "382 0.023151026762521343\n",
      "383 0.022317800930440314\n",
      "384 0.021514543052613203\n",
      "385 0.020740248686111133\n",
      "386 0.01999383580630077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 0.01927426667405193\n",
      "388 0.018580728700601404\n",
      "389 0.01791213964963815\n",
      "390 0.0172675559595269\n",
      "391 0.016646224059452083\n",
      "392 0.016047241679441646\n",
      "393 0.015469883955900845\n",
      "394 0.014913253738205488\n",
      "395 0.01437669675015615\n",
      "396 0.013859551510212573\n",
      "397 0.013361368124543038\n",
      "398 0.01288079104695132\n",
      "399 0.012417337323013512\n",
      "400 0.011970670349500412\n",
      "401 0.011540106354955703\n",
      "402 0.011125030587025467\n",
      "403 0.010724861708617032\n",
      "404 0.01033908307978608\n",
      "405 0.009967220207746697\n",
      "406 0.009608751507723473\n",
      "407 0.009263178890382823\n",
      "408 0.008930009856283673\n",
      "409 0.008608924570869958\n",
      "410 0.008299340337323189\n",
      "411 0.008000877967106065\n",
      "412 0.0077131488332210785\n",
      "413 0.007435800702477659\n",
      "414 0.007168439858031993\n",
      "415 0.006910682388550659\n",
      "416 0.006662187673531372\n",
      "417 0.006422631000814211\n",
      "418 0.006191691901568239\n",
      "419 0.005969081101041418\n",
      "420 0.005754483062981434\n",
      "421 0.0055475804442318005\n",
      "422 0.005348150484141067\n",
      "423 0.005155878443080517\n",
      "424 0.004970538595333605\n",
      "425 0.004791832293900383\n",
      "426 0.004619580018639288\n",
      "427 0.004453518728958565\n",
      "428 0.00429342136256544\n",
      "429 0.004139089496902357\n",
      "430 0.0039902928898525175\n",
      "431 0.0038468667040311463\n",
      "432 0.0037085826605128954\n",
      "433 0.003575280938127188\n",
      "434 0.0034467742731464283\n",
      "435 0.0033228961212197518\n",
      "436 0.00320346248068266\n",
      "437 0.003088321600981831\n",
      "438 0.002977325547596928\n",
      "439 0.0028703296552416283\n",
      "440 0.002767169666077187\n",
      "441 0.0026677161620943446\n",
      "442 0.002571843422931699\n",
      "443 0.002479415239904928\n",
      "444 0.002390308896113502\n",
      "445 0.002304405182699143\n",
      "446 0.002221593373812588\n",
      "447 0.0021417511733052095\n",
      "448 0.002064796884887096\n",
      "449 0.0019905999398590335\n",
      "450 0.0019190640277813088\n",
      "451 0.0018501040937380282\n",
      "452 0.0017836294082420666\n",
      "453 0.001719540522811201\n",
      "454 0.001657748676586334\n",
      "455 0.0015981831402774984\n",
      "456 0.0015407545798749316\n",
      "457 0.0014853923982165137\n",
      "458 0.0014320185576335311\n",
      "459 0.0013805654304403854\n",
      "460 0.0013309585158752385\n",
      "461 0.0012831679715889686\n",
      "462 0.0012370783487061676\n",
      "463 0.0011926317886619168\n",
      "464 0.0011497837332599336\n",
      "465 0.001108477149382951\n",
      "466 0.0010686518441447744\n",
      "467 0.0010302559099973829\n",
      "468 0.0009932432825977742\n",
      "469 0.0009575571929301359\n",
      "470 0.0009231546767194414\n",
      "471 0.0008899875325151991\n",
      "472 0.0008580133845128308\n",
      "473 0.000827189142159044\n",
      "474 0.0007974721555028539\n",
      "475 0.0007688267785497541\n",
      "476 0.0007412064030636021\n",
      "477 0.0007145831397634841\n",
      "478 0.0006889142950587642\n",
      "479 0.0006641649369596874\n",
      "480 0.0006403047418685453\n",
      "481 0.0006173028795106765\n",
      "482 0.0005951281383914666\n",
      "483 0.000573749054802104\n",
      "484 0.0005531384430325033\n",
      "485 0.0005332685912422374\n",
      "486 0.0005141129535682792\n",
      "487 0.000495646028563957\n",
      "488 0.0004778440703016737\n",
      "489 0.0004606785650703792\n",
      "490 0.00044413130191495306\n",
      "491 0.00042817974627390644\n",
      "492 0.00041279968454033397\n",
      "493 0.00039797099066970804\n",
      "494 0.00038367592058782807\n",
      "495 0.0003698947863696212\n",
      "496 0.00035660863837502223\n",
      "497 0.0003437996544496132\n",
      "498 0.0003314502934017008\n",
      "499 0.0003195454488690466\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500): #change as per convenience\n",
    "  # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: NN using Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors; you should think of them as a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph and Autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set requires_grad=True when constructing that Tensor. Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If x is a Tensor with requires_grad=True, then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Sometimes you may wish to prevent PyTorch from building computational graphs when performing certain operations on Tensors with requires_grad=True; for example we usually don't want to backpropagate through the weight update steps when training a neural network. In such scenarios we can use the torch.no_grad() context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/autograd1.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/variable1.png\" width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w1 w.r.t to L: tensor(-36.)\n",
      "Gradient of w2 w.r.t to L: tensor(-28.)\n",
      "Gradient of w3 w.r.t to L: tensor(-8.)\n",
      "Gradient of w4 w.r.t to L: tensor(-20.)\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w\"+str(index)+\" w.r.t to L: \" +str(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
