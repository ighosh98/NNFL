{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/andrej.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras vs PyTorch\n",
    "\n",
    "* Keras is without a doubt the easier option if you want a plug & play framework: to quickly build, train, and evaluate a model, without spending much time on mathematical implementation details.\n",
    "\n",
    "* PyTorch offers a lower-level approach and more flexibility for the more mathematically-inclined users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head to Head\n",
    "\n",
    "Consider this head-to-head comparison of how a simple convolutional network is defined in Keras and PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 10) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        x = F.log_softmax(self.fc1(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply \"neurons.\" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up : NN using Numpy\n",
    "\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 742.7541596357506\n",
      "1 741.8979733544786\n",
      "2 741.043367345029\n",
      "3 740.1903377214019\n",
      "4 739.3388806095695\n",
      "5 738.4889921474314\n",
      "6 737.6406684847705\n",
      "7 736.7939057832097\n",
      "8 735.9487002161668\n",
      "9 735.1050479688124\n",
      "10 734.2629452380262\n",
      "11 733.4223882323525\n",
      "12 732.583373171959\n",
      "13 731.7458962885926\n",
      "14 730.9099538255379\n",
      "15 730.0755420375737\n",
      "16 729.2426571909311\n",
      "17 728.4112955632522\n",
      "18 727.5814534435467\n",
      "19 726.7531271321513\n",
      "20 725.9263129406884\n",
      "21 725.1010071920225\n",
      "22 724.2772062202225\n",
      "23 723.4549063705174\n",
      "24 722.6341039992581\n",
      "25 721.814795473874\n",
      "26 720.9969771728356\n",
      "27 720.1806454856112\n",
      "28 719.3657968126291\n",
      "29 718.552427565236\n",
      "30 717.7405341656581\n",
      "31 716.9301130469614\n",
      "32 716.1211606530112\n",
      "33 715.3136734384352\n",
      "34 714.5076478685809\n",
      "35 713.7030804194794\n",
      "36 712.8999675778059\n",
      "37 712.0983058408403\n",
      "38 711.2980917164298\n",
      "39 710.4993217229496\n",
      "40 709.7019923892658\n",
      "41 708.9061002546958\n",
      "42 708.1116418689725\n",
      "43 707.3186137922048\n",
      "44 706.5270125948417\n",
      "45 705.7368348576335\n",
      "46 704.9480771715953\n",
      "47 704.1607361379706\n",
      "48 703.3748083681926\n",
      "49 702.5902904838491\n",
      "50 701.8071791166461\n",
      "51 701.0254709083697\n",
      "52 700.2451625108514\n",
      "53 699.4662505859309\n",
      "54 698.6887318054213\n",
      "55 697.9126028510723\n",
      "56 697.1378604145348\n",
      "57 696.3645011973254\n",
      "58 695.5925219107918\n",
      "59 694.8219192760761\n",
      "60 694.0526900240808\n",
      "61 693.2848308954335\n",
      "62 692.5183386404526\n",
      "63 691.753210019112\n",
      "64 690.9894418010067\n",
      "65 690.2270307653191\n",
      "66 689.4659737007839\n",
      "67 688.7062674056554\n",
      "68 687.9479086876713\n",
      "69 687.1908943640216\n",
      "70 686.4352212613129\n",
      "71 685.6808862155362\n",
      "72 684.9278860720328\n",
      "73 684.1762176854613\n",
      "74 683.4258779197644\n",
      "75 682.6768636481365\n",
      "76 681.9291717529904\n",
      "77 681.1827991259246\n",
      "78 680.4377426676907\n",
      "79 679.6939992881614\n",
      "80 678.9515659062984\n",
      "81 678.2104394501195\n",
      "82 677.4706168566668\n",
      "83 676.7320950719754\n",
      "84 675.9948710510414\n",
      "85 675.2589417577897\n",
      "86 674.5243041650433\n",
      "87 673.790955254492\n",
      "88 673.0588920166601\n",
      "89 672.3281114508769\n",
      "90 671.5986105652445\n",
      "91 670.8703863766068\n",
      "92 670.1434359105206\n",
      "93 669.417756201223\n",
      "94 668.6933442916015\n",
      "95 667.970197233165\n",
      "96 667.2483120860112\n",
      "97 666.5276859187984\n",
      "98 665.8083158087152\n",
      "99 665.09019884145\n",
      "100 664.3733321111617\n",
      "101 663.6577127204503\n",
      "102 662.9433377803275\n",
      "103 662.2302044101862\n",
      "104 661.518309737773\n",
      "105 660.807650899158\n",
      "106 660.0982250387058\n",
      "107 659.390029309047\n",
      "108 658.6830608710493\n",
      "109 657.9773168937891\n",
      "110 657.2727945545219\n",
      "111 656.5694910386555\n",
      "112 655.8674035397208\n",
      "113 655.166529259344\n",
      "114 654.466865407217\n",
      "115 653.7684092010722\n",
      "116 653.0711578666524\n",
      "117 652.3751086376839\n",
      "118 651.6802587558491\n",
      "119 650.9866054707581\n",
      "120 650.2941460399225\n",
      "121 649.6028777287274\n",
      "122 648.9127978104048\n",
      "123 648.2239035660048\n",
      "124 647.5361922843713\n",
      "125 646.8496612621133\n",
      "126 646.1643078035784\n",
      "127 645.4801292208269\n",
      "128 644.7971228336045\n",
      "129 644.1152859693161\n",
      "130 643.4346159630002\n",
      "131 642.7551101573014\n",
      "132 642.076765902445\n",
      "133 641.3995805562113\n",
      "134 640.7235514839091\n",
      "135 640.0486760583506\n",
      "136 639.3749516598247\n",
      "137 638.7023756760727\n",
      "138 638.0309455022618\n",
      "139 637.3606585409598\n",
      "140 636.691512202111\n",
      "141 636.0235039030088\n",
      "142 635.3566310682731\n",
      "143 634.6908911298235\n",
      "144 634.0262815268552\n",
      "145 633.3627997058142\n",
      "146 632.7004431203727\n",
      "147 632.0392092314039\n",
      "148 631.3790955069585\n",
      "149 630.7200994222395\n",
      "150 630.0622184595782\n",
      "151 629.4054501084095\n",
      "152 628.7497918652491\n",
      "153 628.0952412336685\n",
      "154 627.4417957242708\n",
      "155 626.7894528546683\n",
      "156 626.1382101494564\n",
      "157 625.4880651401934\n",
      "158 624.8390153653729\n",
      "159 624.191058370404\n",
      "160 623.5441917075856\n",
      "161 622.8984129360847\n",
      "162 622.2537196219117\n",
      "163 621.6101093378984\n",
      "164 620.967579663675\n",
      "165 620.3261281856469\n",
      "166 619.6857524969714\n",
      "167 619.0464501975364\n",
      "168 618.4082188939362\n",
      "169 617.7710561994504\n",
      "170 617.1349597340202\n",
      "171 616.4999271242268\n",
      "172 615.865956003269\n",
      "173 615.2330440109404\n",
      "174 614.6011887936086\n",
      "175 613.9703880041923\n",
      "176 613.340639302139\n",
      "177 612.7119403534043\n",
      "178 612.0842888304292\n",
      "179 611.4576824121187\n",
      "180 610.832118783821\n",
      "181 610.2075956373042\n",
      "182 609.5841106707367\n",
      "183 608.9616615886648\n",
      "184 608.3402461019923\n",
      "185 607.7198619279586\n",
      "186 607.1005067901176\n",
      "187 606.4821784183168\n",
      "188 605.8648745486773\n",
      "189 605.2485929235716\n",
      "190 604.6333312916037\n",
      "191 604.0190874075879\n",
      "192 603.4058590325292\n",
      "193 602.7936439336007\n",
      "194 602.1824398841258\n",
      "195 601.5722446635561\n",
      "196 600.9630560574516\n",
      "197 600.3548718574601\n",
      "198 599.7476898612983\n",
      "199 599.14150787273\n",
      "200 598.5363237015478\n",
      "201 597.932135163552\n",
      "202 597.3289400805312\n",
      "203 596.7267362802431\n",
      "204 596.1255215963939\n",
      "205 595.5252938686193\n",
      "206 594.9260509424649\n",
      "207 594.3277906693665\n",
      "208 593.7305109066308\n",
      "209 593.1342095174169\n",
      "210 592.5388843707163\n",
      "211 591.9445333413332\n",
      "212 591.3511543098671\n",
      "213 590.7587451626923\n",
      "214 590.1673037919393\n",
      "215 589.5768280954766\n",
      "216 588.9873159768918\n",
      "217 588.3987653454719\n",
      "218 587.8111741161856\n",
      "219 587.2245402096652\n",
      "220 586.6388615521861\n",
      "221 586.0541360756511\n",
      "222 585.4703617175694\n",
      "223 584.8875364210401\n",
      "224 584.3056581347339\n",
      "225 583.724724812874\n",
      "226 583.1447344152187\n",
      "227 582.5656849070433\n",
      "228 581.9875742591217\n",
      "229 581.4104004477094\n",
      "230 580.8341614545252\n",
      "231 580.2588552667335\n",
      "232 579.6844798769259\n",
      "233 579.1110332831059\n",
      "234 578.5385134886685\n",
      "235 577.9669185023844\n",
      "236 577.3962463383825\n",
      "237 576.826495016132\n",
      "238 576.2576625604256\n",
      "239 575.6897470013623\n",
      "240 575.1227463743301\n",
      "241 574.5566587199885\n",
      "242 573.9914820842523\n",
      "243 573.4272145182742\n",
      "244 572.8638540784282\n",
      "245 572.3013988262924\n",
      "246 571.7398468286324\n",
      "247 571.1791961573849\n",
      "248 570.6194448896405\n",
      "249 570.0605911076277\n",
      "250 569.502632898696\n",
      "251 568.9455683552997\n",
      "252 568.3893955749816\n",
      "253 567.8341126603559\n",
      "254 567.2797177190932\n",
      "255 566.7262088639036\n",
      "256 566.1735842125203\n",
      "257 565.621841887684\n",
      "258 565.0709800171267\n",
      "259 564.5209967335559\n",
      "260 563.9718901746387\n",
      "261 563.4236584829854\n",
      "262 562.8762998061344\n",
      "263 562.3298122965366\n",
      "264 561.7841941115387\n",
      "265 561.2394434133691\n",
      "266 560.6955583691209\n",
      "267 560.152537150737\n",
      "268 559.6103779349953\n",
      "269 559.0690789034923\n",
      "270 558.5286382426287\n",
      "271 557.9890541435932\n",
      "272 557.450324802348\n",
      "273 556.912448419613\n",
      "274 556.3754232008523\n",
      "275 555.8392473562568\n",
      "276 555.3039191007312\n",
      "277 554.769436653878\n",
      "278 554.2357982399831\n",
      "279 553.7030020880009\n",
      "280 553.171046431539\n",
      "281 552.6399295088447\n",
      "282 552.1096495627893\n",
      "283 551.5802048408539\n",
      "284 551.0515935951144\n",
      "285 550.5238140822282\n",
      "286 549.9968645634185\n",
      "287 549.4707433044605\n",
      "288 548.945448575667\n",
      "289 548.4209786518741\n",
      "290 547.897331812427\n",
      "291 547.3745063411657\n",
      "292 546.8525005264111\n",
      "293 546.3313126609509\n",
      "294 545.8109410420253\n",
      "295 545.2913839713138\n",
      "296 544.7726397549202\n",
      "297 544.2547067033593\n",
      "298 543.7375831315435\n",
      "299 543.2212673587678\n",
      "300 542.7057577086982\n",
      "301 542.1910525093556\n",
      "302 541.6771500931036\n",
      "303 541.1640487966351\n",
      "304 540.651746960958\n",
      "305 540.1402429313821\n",
      "306 539.6295350575058\n",
      "307 539.1196216932026\n",
      "308 538.610501196608\n",
      "309 538.1021719301057\n",
      "310 537.5946322603149\n",
      "311 537.0878805580771\n",
      "312 536.5819151984426\n",
      "313 536.076734560658\n",
      "314 535.5723370281526\n",
      "315 535.0687209885252\n",
      "316 534.5658848335327\n",
      "317 534.0638269590753\n",
      "318 533.5625457651847\n",
      "319 533.0620396560112\n",
      "320 532.562307039811\n",
      "321 532.0633463289332\n",
      "322 531.5651559398069\n",
      "323 531.06773429293\n",
      "324 530.5710798128546\n",
      "325 530.0751909281762\n",
      "326 529.5800660715202\n",
      "327 529.0857036795298\n",
      "328 528.5921021928539\n",
      "329 528.0992600561343\n",
      "330 527.6071757179938\n",
      "331 527.1158476310235\n",
      "332 526.6252742517709\n",
      "333 526.1354540407276\n",
      "334 525.646385462318\n",
      "335 525.1580669848853\n",
      "336 524.6704970806815\n",
      "337 524.1836742258542\n",
      "338 523.6975969004352\n",
      "339 523.2122635883284\n",
      "340 522.7276727772979\n",
      "341 522.2438229589563\n",
      "342 521.7607126287529\n",
      "343 521.2783402859618\n",
      "344 520.7967044336705\n",
      "345 520.315803578768\n",
      "346 519.8356362319332\n",
      "347 519.3562009076238\n",
      "348 518.8774961240639\n",
      "349 518.3995204032333\n",
      "350 517.9222722708555\n",
      "351 517.4457502563871\n",
      "352 516.9699528930053\n",
      "353 516.4948787175977\n",
      "354 516.02052627075\n",
      "355 515.5468940967355\n",
      "356 515.0739807435039\n",
      "357 514.6017847626692\n",
      "358 514.1303047095003\n",
      "359 513.6595391429076\n",
      "360 513.1894866254347\n",
      "361 512.7201457232447\n",
      "362 512.251515006111\n",
      "363 511.7835930474058\n",
      "364 511.31637842408935\n",
      "365 510.849869716699\n",
      "366 510.38406550933814\n",
      "367 509.9189643896658\n",
      "368 509.4545649488862\n",
      "369 508.99086578173694\n",
      "370 508.5278654864794\n",
      "371 508.06556266488815\n",
      "372 507.60395592223955\n",
      "373 507.1430438673015\n",
      "374 506.68282511232326\n",
      "375 506.2232982730251\n",
      "376 505.764461968587\n",
      "377 505.3063148216391\n",
      "378 504.848855458251\n",
      "379 504.3920825079215\n",
      "380 503.93599460356813\n",
      "381 503.4805903815172\n",
      "382 503.02586848149315\n",
      "383 502.57182754660914\n",
      "384 502.1184662233561\n",
      "385 501.6657831615927\n",
      "386 501.2137770145359\n",
      "387 500.7624464387505\n",
      "388 500.3117900941385\n",
      "389 499.8618066439307\n",
      "390 499.41249475467515\n",
      "391 498.9638530962281\n",
      "392 498.515880341744\n",
      "393 498.06857516766564\n",
      "394 497.62193625371435\n",
      "395 497.17596228288005\n",
      "396 496.7306519414119\n",
      "397 496.28600391880826\n",
      "398 495.8420169078074\n",
      "399 495.39868960437747\n",
      "400 494.9560207077068\n",
      "401 494.51400892019507\n",
      "402 494.0726529474431\n",
      "403 493.63195149824344\n",
      "404 493.1919032845713\n",
      "405 492.7525070215748\n",
      "406 492.3137614275651\n",
      "407 491.87566522400857\n",
      "408 491.4382171355154\n",
      "409 491.001415889832\n",
      "410 490.56526021783077\n",
      "411 490.1297488535016\n",
      "412 489.6948805339417\n",
      "413 489.2606539993474\n",
      "414 488.8270679930041\n",
      "415 488.3941212612778\n",
      "416 487.96181255360614\n",
      "417 487.5301406224888\n",
      "418 487.09910422347843\n",
      "419 486.6687021151729\n",
      "420 486.2389330592047\n",
      "421 485.80979582023286\n",
      "422 485.3812891659337\n",
      "423 484.953411866993\n",
      "424 484.5261626970953\n",
      "425 484.09954043291725\n",
      "426 483.67354385411693\n",
      "427 483.2481717433261\n",
      "428 482.8234228861418\n",
      "429 482.39929607111674\n",
      "430 481.9757900897513\n",
      "431 481.5529037364845\n",
      "432 481.1306358086856\n",
      "433 480.7089851066461\n",
      "434 480.2879504335698\n",
      "435 479.8675305955659\n",
      "436 479.44772440163945\n",
      "437 479.0285306636831\n",
      "438 478.609948196469\n",
      "439 478.1919758176403\n",
      "440 477.7746123477025\n",
      "441 477.35785661001546\n",
      "442 476.94170743078473\n",
      "443 476.52616363905383\n",
      "444 476.1112240666955\n",
      "445 475.69688754840365\n",
      "446 475.2831529216852\n",
      "447 474.8700190268519\n",
      "448 474.4574847070121\n",
      "449 474.0455488080628\n",
      "450 473.6342101786814\n",
      "451 473.22346767031775\n",
      "452 472.81332013718634\n",
      "453 472.4037664362577\n",
      "454 471.9948054272509\n",
      "455 471.5864359726258\n",
      "456 471.1786569375743\n",
      "457 470.7714671900134\n",
      "458 470.3648656005769\n",
      "459 469.95885104260697\n",
      "460 469.5534223921476\n",
      "461 469.14857852793614\n",
      "462 468.744318331395\n",
      "463 468.34064068662497\n",
      "464 467.9375444803968\n",
      "465 467.5350286021438\n",
      "466 467.1330919439539\n",
      "467 466.73173340056235\n",
      "468 466.3309518693442\n",
      "469 465.9307462503062\n",
      "470 465.5311154460795\n",
      "471 465.1320583619126\n",
      "472 464.733573905663\n",
      "473 464.33566098779045\n",
      "474 463.93831852134895\n",
      "475 463.54154542197955\n",
      "476 463.1453406079032\n",
      "477 462.74970299991287\n",
      "478 462.35463152136657\n",
      "479 461.9601250981799\n",
      "480 461.5661826588188\n",
      "481 461.17280313429205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 460.779985458144\n",
      "483 460.3877285664482\n",
      "484 459.9960313977991\n",
      "485 459.60489289330496\n",
      "486 459.2143119965814\n",
      "487 458.82428765374397\n",
      "488 458.4348188134005\n",
      "489 458.0459044266448\n",
      "490 457.65754344704897\n",
      "491 457.2697348306568\n",
      "492 456.8824775359766\n",
      "493 456.495770523974\n",
      "494 456.1096127580653\n",
      "495 455.7240032041103\n",
      "496 455.3389408304057\n",
      "497 454.95442460767754\n",
      "498 454.5704535090749\n",
      "499 454.18702651016315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500): #change as per convenience\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.dot(w1)\n",
    "  h_relu = np.maximum(h, 0)\n",
    "  y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  # Compute and print loss\n",
    "  loss = np.square(y_pred - y).sum()\n",
    "  print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "  grad_h = grad_h_relu.copy()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: NN using Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors; you should think of them as a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34768548.0\n",
      "1 31551300.0\n",
      "2 33164876.0\n",
      "3 33016912.0\n",
      "4 27827224.0\n",
      "5 18581480.0\n",
      "6 10324608.0\n",
      "7 5259785.5\n",
      "8 2820194.75\n",
      "9 1712625.25\n",
      "10 1187073.5\n",
      "11 904270.0625\n",
      "12 728773.1875\n",
      "13 605760.0\n",
      "14 512347.4375\n",
      "15 438081.125\n",
      "16 377513.9375\n",
      "17 327269.65625\n",
      "18 285114.78125\n",
      "19 249548.5625\n",
      "20 219243.265625\n",
      "21 193288.0625\n",
      "22 170939.234375\n",
      "23 151608.21875\n",
      "24 134826.0\n",
      "25 120195.9921875\n",
      "26 107396.453125\n",
      "27 96160.203125\n",
      "28 86275.640625\n",
      "29 77553.5703125\n",
      "30 69835.9375\n",
      "31 62995.7265625\n",
      "32 56912.8984375\n",
      "33 51499.83984375\n",
      "34 46666.41796875\n",
      "35 42344.2578125\n",
      "36 38472.7734375\n",
      "37 34996.81640625\n",
      "38 31873.78515625\n",
      "39 29063.787109375\n",
      "40 26530.9140625\n",
      "41 24243.96875\n",
      "42 22175.810546875\n",
      "43 20300.99609375\n",
      "44 18602.189453125\n",
      "45 17061.767578125\n",
      "46 15662.4169921875\n",
      "47 14389.748046875\n",
      "48 13231.091796875\n",
      "49 12175.1455078125\n",
      "50 11212.1171875\n",
      "51 10332.736328125\n",
      "52 9529.0966796875\n",
      "53 8794.2255859375\n",
      "54 8121.57421875\n",
      "55 7502.99609375\n",
      "56 6936.109375\n",
      "57 6416.255859375\n",
      "58 5939.1474609375\n",
      "59 5500.90869140625\n",
      "60 5097.92578125\n",
      "61 4727.57470703125\n",
      "62 4386.662109375\n",
      "63 4072.6044921875\n",
      "64 3783.11474609375\n",
      "65 3516.1298828125\n",
      "66 3269.796875\n",
      "67 3042.28759765625\n",
      "68 2832.005859375\n",
      "69 2637.765625\n",
      "70 2458.076171875\n",
      "71 2291.637939453125\n",
      "72 2137.484619140625\n",
      "73 1994.656005859375\n",
      "74 1862.4649658203125\n",
      "75 1739.8548583984375\n",
      "76 1626.030517578125\n",
      "77 1520.2996826171875\n",
      "78 1422.0682373046875\n",
      "79 1330.8079833984375\n",
      "80 1245.9111328125\n",
      "81 1166.8768310546875\n",
      "82 1093.290283203125\n",
      "83 1024.7469482421875\n",
      "84 960.89501953125\n",
      "85 901.40576171875\n",
      "86 845.9293212890625\n",
      "87 794.1639404296875\n",
      "88 745.8297119140625\n",
      "89 700.6994018554688\n",
      "90 658.522705078125\n",
      "91 619.1029052734375\n",
      "92 582.2551879882812\n",
      "93 547.785888671875\n",
      "94 515.5449829101562\n",
      "95 485.3559265136719\n",
      "96 457.08294677734375\n",
      "97 430.6239318847656\n",
      "98 405.81402587890625\n",
      "99 382.5557556152344\n",
      "100 360.7338562011719\n",
      "101 340.252685546875\n",
      "102 321.03643798828125\n",
      "103 302.99029541015625\n",
      "104 286.039306640625\n",
      "105 270.1071472167969\n",
      "106 255.13726806640625\n",
      "107 241.0562286376953\n",
      "108 227.80218505859375\n",
      "109 215.34039306640625\n",
      "110 203.61367797851562\n",
      "111 192.57733154296875\n",
      "112 182.17373657226562\n",
      "113 172.37208557128906\n",
      "114 163.13629150390625\n",
      "115 154.43515014648438\n",
      "116 146.22506713867188\n",
      "117 138.48318481445312\n",
      "118 131.18174743652344\n",
      "119 124.29370880126953\n",
      "120 117.78826904296875\n",
      "121 111.64509582519531\n",
      "122 105.8421630859375\n",
      "123 100.36119079589844\n",
      "124 95.18353271484375\n",
      "125 90.28987121582031\n",
      "126 85.66378021240234\n",
      "127 81.28909301757812\n",
      "128 77.15192413330078\n",
      "129 73.23829650878906\n",
      "130 69.53236389160156\n",
      "131 66.02555084228516\n",
      "132 62.70817947387695\n",
      "133 59.565521240234375\n",
      "134 56.58959197998047\n",
      "135 53.76749801635742\n",
      "136 51.09665298461914\n",
      "137 48.5645751953125\n",
      "138 46.164894104003906\n",
      "139 43.890174865722656\n",
      "140 41.733211517333984\n",
      "141 39.68855667114258\n",
      "142 37.748069763183594\n",
      "143 35.90657424926758\n",
      "144 34.16022872924805\n",
      "145 32.502716064453125\n",
      "146 30.92845344543457\n",
      "147 29.43482780456543\n",
      "148 28.016752243041992\n",
      "149 26.669145584106445\n",
      "150 25.39049530029297\n",
      "151 24.175783157348633\n",
      "152 23.02048110961914\n",
      "153 21.922765731811523\n",
      "154 20.878976821899414\n",
      "155 19.888416290283203\n",
      "156 18.946693420410156\n",
      "157 18.0506591796875\n",
      "158 17.198429107666016\n",
      "159 16.388242721557617\n",
      "160 15.617773056030273\n",
      "161 14.884034156799316\n",
      "162 14.18644905090332\n",
      "163 13.523223876953125\n",
      "164 12.891898155212402\n",
      "165 12.29015827178955\n",
      "166 11.718374252319336\n",
      "167 11.173675537109375\n",
      "168 10.654949188232422\n",
      "169 10.162115097045898\n",
      "170 9.692130088806152\n",
      "171 9.244473457336426\n",
      "172 8.817630767822266\n",
      "173 8.41136646270752\n",
      "174 8.024527549743652\n",
      "175 7.656018257141113\n",
      "176 7.304816246032715\n",
      "177 6.970240592956543\n",
      "178 6.651374340057373\n",
      "179 6.347316741943359\n",
      "180 6.057448863983154\n",
      "181 5.781405448913574\n",
      "182 5.5183916091918945\n",
      "183 5.2673749923706055\n",
      "184 5.027966499328613\n",
      "185 4.800018310546875\n",
      "186 4.5822577476501465\n",
      "187 4.3748016357421875\n",
      "188 4.176889896392822\n",
      "189 3.9882378578186035\n",
      "190 3.8082282543182373\n",
      "191 3.636420965194702\n",
      "192 3.4727749824523926\n",
      "193 3.3162662982940674\n",
      "194 3.1671345233917236\n",
      "195 3.0248398780822754\n",
      "196 2.889127731323242\n",
      "197 2.7594919204711914\n",
      "198 2.6358511447906494\n",
      "199 2.5178585052490234\n",
      "200 2.4052324295043945\n",
      "201 2.297746419906616\n",
      "202 2.195211410522461\n",
      "203 2.097170352935791\n",
      "204 2.003586769104004\n",
      "205 1.914482831954956\n",
      "206 1.8292784690856934\n",
      "207 1.7479853630065918\n",
      "208 1.670352816581726\n",
      "209 1.5961124897003174\n",
      "210 1.5251820087432861\n",
      "211 1.4575363397598267\n",
      "212 1.392937183380127\n",
      "213 1.3310710191726685\n",
      "214 1.2722247838974\n",
      "215 1.2159171104431152\n",
      "216 1.1622183322906494\n",
      "217 1.1108193397521973\n",
      "218 1.0617287158966064\n",
      "219 1.014877200126648\n",
      "220 0.9701048731803894\n",
      "221 0.9272637367248535\n",
      "222 0.8863467574119568\n",
      "223 0.8474709987640381\n",
      "224 0.8101550936698914\n",
      "225 0.7745157480239868\n",
      "226 0.7404836416244507\n",
      "227 0.7078453302383423\n",
      "228 0.6767838001251221\n",
      "229 0.6470241546630859\n",
      "230 0.6187208890914917\n",
      "231 0.5915647745132446\n",
      "232 0.565517008304596\n",
      "233 0.5407774448394775\n",
      "234 0.5171367526054382\n",
      "235 0.49447956681251526\n",
      "236 0.4727935194969177\n",
      "237 0.4520791172981262\n",
      "238 0.43237778544425964\n",
      "239 0.41337496042251587\n",
      "240 0.3953015208244324\n",
      "241 0.37809014320373535\n",
      "242 0.3616195321083069\n",
      "243 0.3458557724952698\n",
      "244 0.3308010697364807\n",
      "245 0.3163929581642151\n",
      "246 0.3025432527065277\n",
      "247 0.28937286138534546\n",
      "248 0.27680233120918274\n",
      "249 0.2647823393344879\n",
      "250 0.253307580947876\n",
      "251 0.24221765995025635\n",
      "252 0.23166564106941223\n",
      "253 0.2216351479291916\n",
      "254 0.21195150911808014\n",
      "255 0.20274478197097778\n",
      "256 0.1939511001110077\n",
      "257 0.1854969561100006\n",
      "258 0.17745091021060944\n",
      "259 0.16974930465221405\n",
      "260 0.16239720582962036\n",
      "261 0.15538428723812103\n",
      "262 0.14866234362125397\n",
      "263 0.1422061026096344\n",
      "264 0.136040598154068\n",
      "265 0.13016334176063538\n",
      "266 0.1245255395770073\n",
      "267 0.11912494897842407\n",
      "268 0.11396406590938568\n",
      "269 0.10905921459197998\n",
      "270 0.10433628410100937\n",
      "271 0.09981928765773773\n",
      "272 0.09548933058977127\n",
      "273 0.09137091040611267\n",
      "274 0.08746244758367538\n",
      "275 0.08366338163614273\n",
      "276 0.08006617426872253\n",
      "277 0.07660262286663055\n",
      "278 0.07330793142318726\n",
      "279 0.07016414403915405\n",
      "280 0.06711266189813614\n",
      "281 0.0642126053571701\n",
      "282 0.06143517047166824\n",
      "283 0.05881392955780029\n",
      "284 0.05628824234008789\n",
      "285 0.053841762244701385\n",
      "286 0.05155419185757637\n",
      "287 0.04933524131774902\n",
      "288 0.04721162095665932\n",
      "289 0.04518813639879227\n",
      "290 0.043237652629613876\n",
      "291 0.04139263555407524\n",
      "292 0.03962679207324982\n",
      "293 0.03794875368475914\n",
      "294 0.03631456196308136\n",
      "295 0.03475189581513405\n",
      "296 0.03326190263032913\n",
      "297 0.031848203390836716\n",
      "298 0.030483650043606758\n",
      "299 0.0291910283267498\n",
      "300 0.027948053553700447\n",
      "301 0.026755433529615402\n",
      "302 0.025598254054784775\n",
      "303 0.02451511099934578\n",
      "304 0.02347644791007042\n",
      "305 0.02247493714094162\n",
      "306 0.021527495235204697\n",
      "307 0.02062966674566269\n",
      "308 0.019746193662285805\n",
      "309 0.018905149772763252\n",
      "310 0.018113302066922188\n",
      "311 0.017339784651994705\n",
      "312 0.016604948788881302\n",
      "313 0.015901006758213043\n",
      "314 0.015240006148815155\n",
      "315 0.014598225243389606\n",
      "316 0.013985155150294304\n",
      "317 0.013391165994107723\n",
      "318 0.012826070189476013\n",
      "319 0.012289361096918583\n",
      "320 0.011772464960813522\n",
      "321 0.011279907077550888\n",
      "322 0.010808650404214859\n",
      "323 0.010354667901992798\n",
      "324 0.009930114261806011\n",
      "325 0.00951637327671051\n",
      "326 0.009114880114793777\n",
      "327 0.008738483302295208\n",
      "328 0.008375521749258041\n",
      "329 0.00802827998995781\n",
      "330 0.007700913120061159\n",
      "331 0.007388575933873653\n",
      "332 0.0070845382288098335\n",
      "333 0.006791871972382069\n",
      "334 0.006512218154966831\n",
      "335 0.00624418631196022\n",
      "336 0.005990743637084961\n",
      "337 0.0057488130405545235\n",
      "338 0.005518484860658646\n",
      "339 0.005298386327922344\n",
      "340 0.005082977470010519\n",
      "341 0.004881301894783974\n",
      "342 0.00468929810449481\n",
      "343 0.004502264317125082\n",
      "344 0.00432228110730648\n",
      "345 0.004151929169893265\n",
      "346 0.0039863125421106815\n",
      "347 0.0038244512397795916\n",
      "348 0.003677315777167678\n",
      "349 0.003535031108185649\n",
      "350 0.003399786539375782\n",
      "351 0.0032679815776646137\n",
      "352 0.003141548251733184\n",
      "353 0.0030228658579289913\n",
      "354 0.0029070270247757435\n",
      "355 0.002795704174786806\n",
      "356 0.0026922929100692272\n",
      "357 0.0025852748658508062\n",
      "358 0.002489309059455991\n",
      "359 0.0023934158962219954\n",
      "360 0.002306189853698015\n",
      "361 0.0022227608133107424\n",
      "362 0.0021436577662825584\n",
      "363 0.002064037136733532\n",
      "364 0.0019855520222336054\n",
      "365 0.0019133479800075293\n",
      "366 0.0018480250146239996\n",
      "367 0.0017796835163608193\n",
      "368 0.001713078236207366\n",
      "369 0.0016563736135140061\n",
      "370 0.0015977576840668917\n",
      "371 0.001543001621030271\n",
      "372 0.0014888314763084054\n",
      "373 0.0014383140951395035\n",
      "374 0.001388411270454526\n",
      "375 0.0013430803082883358\n",
      "376 0.001293556415475905\n",
      "377 0.0012503834441304207\n",
      "378 0.0012104922207072377\n",
      "379 0.0011684889905154705\n",
      "380 0.0011320679914206266\n",
      "381 0.001095229061320424\n",
      "382 0.0010594436898827553\n",
      "383 0.0010250144405290484\n",
      "384 0.0009933568071573973\n",
      "385 0.0009613214642740786\n",
      "386 0.0009300379315391183\n",
      "387 0.0009005471947602928\n",
      "388 0.0008730905828997493\n",
      "389 0.0008458563825115561\n",
      "390 0.0008176065748557448\n",
      "391 0.0007939665229059756\n",
      "392 0.0007715537794865668\n",
      "393 0.000748717226088047\n",
      "394 0.0007255651289597154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 0.0007036046590656042\n",
      "396 0.000682456127833575\n",
      "397 0.0006622145883738995\n",
      "398 0.0006434269016608596\n",
      "399 0.0006244323449209332\n",
      "400 0.0006048176437616348\n",
      "401 0.0005881496472284198\n",
      "402 0.0005728085525333881\n",
      "403 0.0005575393442995846\n",
      "404 0.0005416148924268782\n",
      "405 0.0005273196729831398\n",
      "406 0.000512141443323344\n",
      "407 0.0004989765584468842\n",
      "408 0.0004839517641812563\n",
      "409 0.0004723244928754866\n",
      "410 0.0004591959295794368\n",
      "411 0.00044632560457102954\n",
      "412 0.0004354284901637584\n",
      "413 0.00042318148189224303\n",
      "414 0.00041317573050037026\n",
      "415 0.00040257518412545323\n",
      "416 0.00039167635259218514\n",
      "417 0.00038164720172062516\n",
      "418 0.0003712526522576809\n",
      "419 0.0003628657723311335\n",
      "420 0.0003529731184244156\n",
      "421 0.00034392467932775617\n",
      "422 0.0003365102456882596\n",
      "423 0.0003277621290180832\n",
      "424 0.0003196651232428849\n",
      "425 0.0003129307006020099\n",
      "426 0.0003050506638828665\n",
      "427 0.0002984178136102855\n",
      "428 0.0002910658367909491\n",
      "429 0.00028413295513018966\n",
      "430 0.0002787251432891935\n",
      "431 0.0002720911870710552\n",
      "432 0.0002658977173268795\n",
      "433 0.0002588307543192059\n",
      "434 0.00025325152091681957\n",
      "435 0.0002481125120539218\n",
      "436 0.0002431792818242684\n",
      "437 0.00023718734155409038\n",
      "438 0.00023148965556174517\n",
      "439 0.0002263686910737306\n",
      "440 0.00022231010370887816\n",
      "441 0.00021756591740995646\n",
      "442 0.00021243107039481401\n",
      "443 0.00020796914759557694\n",
      "444 0.000203929579583928\n",
      "445 0.0001995406491914764\n",
      "446 0.00019616023928392678\n",
      "447 0.00019172122119925916\n",
      "448 0.00018801577971316874\n",
      "449 0.00018385189468972385\n",
      "450 0.00018079468281939626\n",
      "451 0.00017667977954261005\n",
      "452 0.00017341304919682443\n",
      "453 0.00017083068087231368\n",
      "454 0.00016722858708817512\n",
      "455 0.00016412217519246042\n",
      "456 0.000161388743435964\n",
      "457 0.00015826750313863158\n",
      "458 0.00015462064766325057\n",
      "459 0.00015192975115496665\n",
      "460 0.0001497067860327661\n",
      "461 0.00014674612611997873\n",
      "462 0.0001445312809664756\n",
      "463 0.00014216906856745481\n",
      "464 0.00013907416723668575\n",
      "465 0.0001366601645713672\n",
      "466 0.00013376260176301003\n",
      "467 0.00013149649021215737\n",
      "468 0.0001296890841331333\n",
      "469 0.00012724939733743668\n",
      "470 0.0001248382031917572\n",
      "471 0.00012284197146072984\n",
      "472 0.00012050857185386121\n",
      "473 0.00011866980639752\n",
      "474 0.00011691798135871068\n",
      "475 0.00011489002645248547\n",
      "476 0.00011314144649077207\n",
      "477 0.0001115749473683536\n",
      "478 0.00010955923062283546\n",
      "479 0.0001076691405614838\n",
      "480 0.0001057253684848547\n",
      "481 0.00010406237561255693\n",
      "482 0.00010234254295937717\n",
      "483 0.00010077303159050643\n",
      "484 9.93476205621846e-05\n",
      "485 9.782385313883424e-05\n",
      "486 9.621055505704135e-05\n",
      "487 9.476044215261936e-05\n",
      "488 9.344908175989985e-05\n",
      "489 9.210149437421933e-05\n",
      "490 9.047061757883057e-05\n",
      "491 8.923580753616989e-05\n",
      "492 8.798402268439531e-05\n",
      "493 8.666426583658904e-05\n",
      "494 8.53825404192321e-05\n",
      "495 8.396951307076961e-05\n",
      "496 8.270333637483418e-05\n",
      "497 8.136980613926426e-05\n",
      "498 8.02486902102828e-05\n",
      "499 7.921887299744412e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph and Autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set requires_grad=True when constructing that Tensor. Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If x is a Tensor with requires_grad=True, then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Sometimes you may wish to prevent PyTorch from building computational graphs when performing certain operations on Tensors with requires_grad=True; for example we usually don't want to backpropagate through the weight update steps when training a neural network. In such scenarios we can use the torch.no_grad() context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/autograd1.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/variable1.png\" width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w1 w.r.t to L: -36.0\n",
      "Gradient of w2 w.r.t to L: -28.0\n",
      "Gradient of w3 w.r.t to L: -8.0\n",
      "Gradient of w4 w.r.t to L: -20.0\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(f\"Gradient of w{index} w.r.t to L: {gradient}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
